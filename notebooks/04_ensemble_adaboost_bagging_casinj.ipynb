{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04_ensemble_adaboost_bagging_casinj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Qte-lOchr6K"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv('datawithTime.csv')\n",
    "\n",
    "feature_columns = ['CARS', 'TEMP', 'TRNSPD', 'TONS', 'POSITON1', 'HEADEND1', 'LOADF1', 'EMPTYF1', 'HIGHSPD', 'hour', 'minute', 'RAILROAD', 'YEAR', 'MONTH', 'DAY', 'STATE ', 'VISIBLTY', 'WEATHER', 'TYPEQ', 'TRKCLAS', 'TYPTRK', 'CAUSE', 'ACCTRK']\n",
    "target_column = 'CASINJ'\n",
    "\n",
    "X = data[feature_columns]\n",
    "y = data[target_column]\n",
    "\n",
    "def initialize_weights(num_samples):\n",
    "    return np.ones(num_samples) / num_samples\n",
    "\n",
    "def train_weak_classifier(X, y, weights):\n",
    "    classifier = DecisionTreeClassifier(max_depth=1)\n",
    "    classifier.fit(X, y, sample_weight=weights)\n",
    "    return classifier\n",
    "\n",
    "def calculate_error_rate(classifier, X, y, weights):\n",
    "    predictions = classifier.predict(X)\n",
    "    incorrect = predictions != y\n",
    "    error_rate = np.sum(weights * incorrect)\n",
    "    return error_rate\n",
    "\n",
    "def calculate_classifier_weight(error_rate):\n",
    "    gamma = 0.5 * np.log((1 - error_rate) / error_rate)\n",
    "    return gamma\n",
    "\n",
    "def update_weights(weights, classifier, gamma, X, y):\n",
    "    predictions = classifier.predict(X)\n",
    "    incorrect = predictions != y\n",
    "    update_factor = np.exp(-gamma * incorrect)\n",
    "    new_weights = weights * update_factor / np.sum(weights * np.exp(-gamma * y * classifier.predict(X)))\n",
    "    return new_weights\n",
    "\n",
    "def adaboost(X, y, num_rounds):\n",
    "    num_samples = len(X)\n",
    "    weights = initialize_weights(num_samples)\n",
    "    classifiers = []\n",
    "    alphas = []\n",
    "\n",
    "    for round_num in range(num_rounds):\n",
    "        print(f\"\\nBoosting Round {round_num + 1}\")\n",
    "\n",
    "        classifier = train_weak_classifier(X, y, weights)\n",
    "\n",
    "        error_rate = calculate_error_rate(classifier, X, y, weights)\n",
    "        print(f\"Error rate: {error_rate:.4f}\")\n",
    "\n",
    "        gamma = calculate_classifier_weight(error_rate)\n",
    "        print(f\"Classifier weight (γk): {gamma:.4f}\")\n",
    "\n",
    "        weights = update_weights(weights, classifier, gamma, X, y)\n",
    "\n",
    "        classifiers.append(classifier)\n",
    "        alphas.append(gamma)\n",
    "\n",
    "    def strong_classifier(X):\n",
    "        total = np.zeros(len(X))\n",
    "        for alpha, classifier in zip(alphas, classifiers):\n",
    "            total += alpha * classifier.predict(X)\n",
    "        return np.sign(total)\n",
    "\n",
    "    return strong_classifier\n",
    "\n",
    "K = 5  # You can adjust K as needed\n",
    "alpha = 0.2\n",
    "\n",
    "strong_classifiers = []\n",
    "\n",
    "for _ in range(K):\n",
    "    X_subset, _, y_subset, _ = train_test_split(X, y, train_size=alpha, random_state=None)\n",
    "\n",
    "    weights = initialize_weights(len(X_subset))\n",
    "\n",
    "    num_rounds = 10  # You can adjust the number of boosting rounds\n",
    "    strong_classifier = adaboost(X_subset, y_subset, num_rounds)\n",
    "\n",
    "    strong_classifiers.append(strong_classifier)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv('datawithTime.csv')\n",
    "\n",
    "feature_columns = ['CARS', 'TEMP', 'TRNSPD', 'TONS', 'POSITON1', 'HEADEND1', 'LOADF1', 'EMPTYF1', 'HIGHSPD', 'hour', 'minute', 'RAILROAD', 'YEAR', 'MONTH', 'DAY', 'STATE ', 'VISIBLTY', 'WEATHER', 'TYPEQ', 'TRKCLAS', 'TYPTRK', 'CAUSE', 'ACCTRK']\n",
    "target_column = 'CASINJ'\n",
    "\n",
    "X = data[feature_columns]\n",
    "y = data[target_column]\n",
    "\n",
    "def initialize_weights(num_samples):\n",
    "    print(num_samples)\n",
    "    return np.ones(num_samples) / num_samples\n",
    "\n",
    "def train_weak_classifier(X, y, weights):\n",
    "    classifier = DecisionTreeClassifier(max_depth=1)\n",
    "    classifier.fit(X, y, sample_weight=weights)\n",
    "    return classifier\n",
    "\n",
    "def calculate_error_rate(classifier, X, y, weights):\n",
    "    predictions = classifier.predict(X)\n",
    "    incorrect = predictions != y\n",
    "    error_rate = np.sum(weights * incorrect)\n",
    "    print(error_rate)\n",
    "    return error_rate\n",
    "\n",
    "def calculate_classifier_weight(error_rate):\n",
    "    gamma = 0.5 * np.log((1 - error_rate) / error_rate)\n",
    "    print(gamma)\n",
    "    return gamma\n",
    "\n",
    "def update_weights(weights, classifier, gamma, X, y):\n",
    "    predictions = classifier.predict(X)\n",
    "    incorrect = predictions != y\n",
    "    update_factor = np.exp(-gamma * incorrect)\n",
    "    new_weights = weights * update_factor / np.sum(weights * np.exp(-gamma * incorrect))\n",
    "    print(new_weights)\n",
    "    return new_weights\n",
    "\n",
    "def adaboost_one_vs_all(X, y, num_classes, num_rounds):\n",
    "    num_samples = len(X)\n",
    "    classifiers = []\n",
    "    alphas = []\n",
    "\n",
    "    for class_label in range(num_classes):\n",
    "        print(f\"\\nTraining AdaBoost for Class {class_label}\")\n",
    "        y_binary = (y == class_label).astype(int)\n",
    "\n",
    "        weights = initialize_weights(num_samples)\n",
    "        class_classifiers = []\n",
    "        class_alphas = []\n",
    "\n",
    "        for round_num in range(num_rounds):\n",
    "            print(f\"\\nBoosting Round {round_num + 1}\")\n",
    "\n",
    "            classifier = train_weak_classifier(X, y_binary, weights)\n",
    "\n",
    "            error_rate = calculate_error_rate(classifier, X, y_binary, weights)\n",
    "            print(f\"Error rate: {error_rate:.4f}\")\n",
    "\n",
    "            gamma = calculate_classifier_weight(error_rate)\n",
    "            print(f\"Classifier weight (γk): {gamma:.4f}\")\n",
    "\n",
    "            weights = update_weights(weights, classifier, gamma, X, y_binary)\n",
    "\n",
    "            class_classifiers.append(classifier)\n",
    "            class_alphas.append(gamma)\n",
    "\n",
    "        classifiers.append(class_classifiers)\n",
    "        alphas.append(class_alphas)\n",
    "\n",
    "    def strong_classifiers(X):\n",
    "        class_votes = []\n",
    "        for class_label, class_classifiers in enumerate(classifiers):\n",
    "            total = np.zeros(len(X))\n",
    "            for alpha, classifier in zip(alphas[class_label], class_classifiers):\n",
    "                total += alpha * classifier.predict(X)\n",
    "            class_votes.append(total)\n",
    "        return np.argmax(class_votes, axis=0)\n",
    "\n",
    "    return strong_classifiers\n",
    "\n",
    "K = 10  # You can adjust K as needed\n",
    "alpha = 0.2\n",
    "\n",
    "num_classes = len(np.unique(y))\n",
    "\n",
    "strong_classifiers_one_vs_all = []\n",
    "\n",
    "for _ in range(K):\n",
    "    X_subset, _, y_subset, _ = train_test_split(X, y, train_size=alpha, random_state=None)\n",
    "\n"
   ],
   "metadata": {
    "id": "8mkVhSErfYxz",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695179038817,
     "user_tz": 240,
     "elapsed": 598,
     "user": {
      "displayName": "zeinab vosooghi",
      "userId": "04640235590116346966"
     }
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv('datawithTime.csv')\n",
    "\n",
    "feature_columns = ['CARS', 'TEMP', 'TRNSPD', 'TONS', 'POSITON1', 'HEADEND1', 'LOADF1', 'EMPTYF1', 'HIGHSPD', 'hour', 'minute', 'RAILROAD', 'YEAR', 'MONTH', 'DAY', 'STATE ', 'VISIBLTY', 'WEATHER', 'TYPEQ', 'TRKCLAS', 'TYPTRK', 'CAUSE', 'ACCTRK']\n",
    "target_column = 'CASINJ'\n",
    "\n",
    "X = data[feature_columns]\n",
    "y = data[target_column]\n",
    "\n",
    "def train_weak_classifier(X, y):\n",
    "    classifier = DecisionTreeClassifier(max_depth=1)\n",
    "    classifier.fit(X, y)\n",
    "    return classifier\n",
    "\n",
    "def calculate_error_rate(classifier, X, y):\n",
    "    predictions = classifier.predict(X)\n",
    "    incorrect = predictions != y\n",
    "    error_rate = np.mean(incorrect)  # Use mean error rate for SAMME\n",
    "    return error_rate\n",
    "\n",
    "def calculate_classifier_weight(error_rate, num_classes):\n",
    "    return 0.5 * np.log((1.0 - error_rate) / (error_rate * (num_classes - 1)))\n",
    "\n",
    "def samme(X, y, num_rounds, num_classes):\n",
    "    num_samples = len(X)\n",
    "    classifiers = []\n",
    "    alphas = []\n",
    "\n",
    "    for round_num in range(num_rounds):\n",
    "        print(f\"\\nBoosting Round {round_num + 1}\")\n",
    "\n",
    "        classifier = train_weak_classifier(X, y)\n",
    "\n",
    "        error_rate = calculate_error_rate(classifier, X, y)\n",
    "        print(f\"Error rate: {error_rate:.4f}\")\n",
    "\n",
    "        gamma = calculate_classifier_weight(error_rate, num_classes)\n",
    "        print(f\"Classifier weight (γk): {gamma:.4f}\")\n",
    "\n",
    "        classifiers.append(classifier)\n",
    "        alphas.append(gamma)\n",
    "\n",
    "    def strong_classifier(X):\n",
    "        num_samples = len(X)\n",
    "        num_classifiers = len(classifiers)\n",
    "        class_scores = np.zeros((num_samples, num_classes))\n",
    "\n",
    "        for i in range(num_classifiers):\n",
    "            classifier = classifiers[i]\n",
    "            alpha = alphas[i]\n",
    "            predictions = classifier.predict(X)\n",
    "            class_scores += alpha * (predictions.reshape(-1, 1) == np.arange(num_classes))\n",
    "\n",
    "        predicted_classes = np.argmax(class_scores, axis=1)\n",
    "        return predicted_classes\n",
    "\n",
    "    return strong_classifier\n",
    "\n",
    "K = 5  # You can adjust K as needed\n",
    "alpha = 0.2\n",
    "num_classes = len(np.unique(y))  # Determine the number of classes in your dataset\n",
    "\n",
    "strong_classifiers = []\n",
    "\n",
    "for _ in range(K):\n",
    "    X_subset, _, y_subset, _ = train_test_split(X, y, train_size=alpha, random_state=None)\n",
    "\n",
    "    num_rounds = 10  # You can adjust the number of boosting rounds\n",
    "    strong_classifier = samme(X_subset, y_subset, num_rounds, num_classes)\n",
    "\n",
    "    strong_classifiers.append(strong_classifier)\n",
    "\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aM-gDCvwkDxv",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695179399100,
     "user_tz": 240,
     "elapsed": 1160,
     "user": {
      "displayName": "zeinab vosooghi",
      "userId": "04640235590116346966"
     }
    },
    "outputId": "0dfed9cb-386f-4ba5-e0aa-e21923b698c4"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}